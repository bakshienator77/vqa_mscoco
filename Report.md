
Gdrive link to all necessary files: https://drive.google.com/drive/folders/1YuKpbWJ9GRXwwbQLqvm4rL_nA0m0uNZ6?usp=sharing

## Custome VQA Dataloader


**Data loader note: Different from the word-level question embedding, the answer embedding is sentence-level (one ID per sentence).**

Because we are treating it as a classification problem and we have a finite set of possible answers. Questions also tend to be longer and more diverse, for eg. "yes" is one of the possible answers but there is no question that is is just one token long.

Finally, if we had encoded answers the same way we would either generate a fixed length answer at the other end which would be a massive tensor if we remained with the one hot representation (ANS_LEN x ANS_DICT_SIZE) OR we would have to sequentially generate the answer at the other end with an RNN. Both of which would be harder and more difficult to frame as a classification problem.

**Data loader note: `__len__` function of the `VqaDataset` class is the**

Number of questions (note NOT unique questions, just on the basis of question ID). Each image can have more than one question associated with it, hence it is not an appropriate measure of size of dataset. Each answer can map to multiple questions and hence it is not an appropriate dataset size. 

**How I you handle questions of different lengths?**

You use the `self._max_question_length` parameter in order to standardise question length, any longer questions are clipped while shorter questions have zero padding.

Output Dimension: `(self._max_question_length, self.question_word_list_length)`

## Simple Baseline

implement the simple method described in [2].

**2.1 This paper uses 'bag-of-words' for question representation. What are the advantage and disadvantage of this type of representation? How do you convert the one-hot encoding loaded in question 1.9 to 'bag-of-words'?**

The advantage is that BOW is compatible with the standard MLP setup where the input dimension is fixed. It can handles sentences or even documents of any length.

The disadvantage is that it discards all information regarding ordering. This is quite important as sentences with very different meanings become the same like "It was no good." and "No, it was good."

To convert one-hot encoding loaded in Q1.9 one just needs to take a sum along the appropriate dimension and the resultant will be a BOW vector with counts of each of the words present in the sentence.

**2.2 What are the 3 major components of the network used in this paper? What are the dimensions of input and output for each of them (including batch size)? In student_code/simple_baseline_net.py, implement the network structure.**

The three components are the image feature extractor (GoogLeNet), word feature extractor (MLP) and (softmax) classifier.

**image feature extractor:**

Input: (B x 3 x H x W) = (B x 3 x 224 x 224)

Output: (B x 1024) (I commented out the final softmax layer)

**word feature extractor:**

Input: (B x BOW_dict_size) = (B x 5747)

Output: (B x word_emb_dim) = (B x 1024) (selected by me)

**classifier:**

Input: (B x (1024+word_emb_dim)) = (B x 2048)

Output: (B x answer_list_length) = (B x 5217)

**2.4 In `student_code/simple_baseline_experiment_runner.py`, specify the arguments `question_word_to_id_map` and `answer_to_id_map` passed into VqaDataset. Explain how you are handling the training set and validation set differently.**

I use the training dataset to create the `question_word_to_id_map` and `answer_to_id_map` by passing those arguments as `None`. For the validation set I pass the ID maps generated by the training set directly so that the mapping is consistent.

**2.5 We recommend a learning rate of 0.8 for word embedding layer and 0.01 for softmax layer, both with SGD optimizer. Explain how this is achieved in your implementation.**

I pass a layer wise learning rate as follows.

```
torch.optim.SGD([{"params": model.word_feature_extractor.parameters(), "lr": 0.8}, {"params": model.classifier.parameters()}], lr = 0.01)
```

**2.7  What loss function do you use?**

cross entropy with logits

**2.10 Describe anything special about your implementation in the report. Include your figures of training loss and validation accuracy. Also show input, prediction and ground truth in 3 different iterations.**

I have plotted validation accuracy on 1k examples every 250 iterations as suggested. best accuracy 1k subset of 0.4972 was saved during th course of training and is at the google drive link. best accuracy full test set was 0.4865 using this best model.

Reproduce results (Training and plots): 

```python -m student_code.main --log_validation --model simple```

Reporduce results (Final Validation accuracy):

```python -m student_code.main --model simple --eval_mode --load_checkpoint best_model_baseline```

**Note:** Make sure checkpoint file is located at ./checkpoints/

![Screenshot%20from%202022-04-22%2014-35-31.png](attachment:Screenshot%20from%202022-04-22%2014-35-31.png)

![Screenshot%20from%202022-04-22%2014-34-48.png](attachment:Screenshot%20from%202022-04-22%2014-34-48.png)

Iter 0 | Iter 250 | Iter 500
- | - | -
![iter0](./iter0.png) | ![iter0](./iter250.png) | ![iter0](./iter500.png)

We can see that in the 0th iteration when the validation accuracy is 0 the GT for the image is `2` but the prediction is `slippery when wet`, however from teration 250 when validation accuracy is 30+ it is already getting this example right and predicts it correctly as `2`. Same for iteration 500.

**3.1 Set up transform used in the Co-attention paper. The transform should be similar to question 2.3, except a different input size. What is the input size. used in the Co-Attention paper [3]?**

The Resnet takes in images of size 448 x 448. The resnet features are of size B x 512 x 7 x 7.

The alternating attention module expects inputs in the shape of B x X x 512 where X depends on whether they are question features (27 = question length) or image features (49 = H\*W of output image). The module outputs attention vectors for question and image for each item in the batch (B x 512 = k).

The question feature extractor expects the question tensor in the shape B x question length x dictionary size = B x 26 x 5747. The output is of size B x 26 x 512 = embedding size.

**3.3.1 What are the three levels in the hierarchy of question representation? How do you obtain each level of representation?**

word level, phrase level and sentence level

The word level is obtained simply by going from the one hot word vectors through an embedding layer to a tanh. 

The phrase level representation is obtained by using convolutional layers with kernel sizes of 1 ,2 and 3 on the embeddings. At this point the three tensors are (B x E x T) shape and a maxpool is taken across them to get a (B x E x T) tensor that takes the most prominent feature from each of the three phrase level representations. The dimensions are permuted back to (B x T x E) before passing it back.

The sentence level representation is also straightforward like the word level and is simply obtained by passing the word embeddings through an LSTM.

**3.3.2 What is attention? How does the co-attention mechanism work? Why do you think it can help with the VQA task?**

Attention is a mechanism by which a deep learning model is made to 'focus' on different part of the input depending on what it is predicting.

The co-attention mechanism has two versions: parallel and alternating. In the parallel version a single attention map $C$ is created (for each level of question hierarchy) such that $C$ maps from question attention space to image attention space and $C^T$ does the reverse. In the alternating version first the question is converted to an attention vector which is used to create the image attention vector, this image attention vetor is then used as guidance to create the final question attention vector.
The benefit of the alternating coattention is that I believe it is more expressive as the the same map does not need to serve both directions.

I think it can help with the VQA task because depending on the question the model should `look` at different parts of the image rather than equally looking at every part of the entire image. Similarly depending on the image, the parts of the question to focus on also plays a part.

**3.3.3 Compared to networks we use in previous assignments, the co-attention network is quite complicated. How do you modularize your code so that it is easy to manage and reuse?**

Taking a cue from how the provided code was broken down it was useful to break the network down into components like image feature encoder, question embedding encoder, classifier, attention module, etc. so that it was easy to swap out compenents or write the forward function in an easy readable way.

Another reason this was useful was that the dataloader expected an encoder and we could even run the problem of task 1 using resnet feature with minimal changes.

**3.5  Similar to question 2.10, describe anything special about your implementation in the report. Include your figures of training loss and validation accuracy. Compare the performance of co-attention network to the simple baseline.**

I have plotted validation accuracy on 1k examples every 250 iterations as suggested. best accuracy on 1k subset of 0.6018 (Epoch 12 iteration 1192) was saved during the course of training and is at the google drive link. best accuracy full test set was 0.5987 using this best model.

I used Adam optimizer with a learning rate of 4e-4. Also my batch_size was 100 as I use a small GPU (only 4 GB of memory).

Reproduce results (Training and plots): 

```python -m student_code.main --log_validation --model coattention --cache_location="./cache"```

Reporduce results (Final Validation accuracy):

```python -m student_code.main --model coattention --eval_mode --load_checkpoint best_model_coattention --cache_location="./cache"```

**Note:** Make sure checkpoint file is located at ./checkpoints/

![Screenshot%20from%202022-04-22%2014-53-10.png](attachment:Screenshot%20from%202022-04-22%2014-53-10.png)

![Screenshot%20from%202022-04-22%2014-54-30.png](attachment:Screenshot%20from%202022-04-22%2014-54-30.png)

Iter 0 | Iter 250 | Iter 500
- | - | -
![iter0](./caiter0.png) | ![iter0](./caiter250.png) | ![iter0](./caiter500.png)

We can see that in the 0th iteration when the validation accuracy is 20% (surprisingly good probably on account of the resnet features!) the GT for the image is `2` but the prediction is `ribbon`. Even after astronomical improvement in 250 iters to 42% accuracy it gets it wrong even then. However from teration 500 when validation accuracy is 43%+ it correctly as `2`.

**4.1 List a few ideas you think of (at least 3, the more the better).**

1. I would try Visual transformers (for images) combines with regular transformers (for question text) in order to see if performance can be improved
2. Another idea I have is that only 86% of all answers are covered with the current limited set of answers. To overcome this I would try a heirarchical approach. That is, when for all the examples predicted to be in the unknown class I'll train another smaller model with the remaining answer classes. So everything remains the same as any of the above two approaches we did but it allows us to cover more classes.
3. Devising a loss function for the next idea I have gets harder as generative text models are capable of being very expressive but the idea is that instead of posing it as a classification problem we pose it as a text generation problem wherein the answer needs to be generated by an RNN or transformer model (preferably pretrained).
4. The last idea I had was that hierarchical question embedding was really overkill, and with a short sentence length like 26 the lstm should be capable of encoding the entire question information and coupled with attention it should be able to perform well.

**(bonus) 4.2 Implementing at least one of the ideas. If you tweak one of your existing implementations, please copy the network to a new, clearly named file before changing it. Include the training loss and test accuracy graphs for your idea.**

I tried idea #4 from the above list and the modification to my CoattentionNet was very simple. I just made a new class that inherited directly from the original as follows but only uses sentence encodings.

I used Adam optimizer with a learning rate of 4e-4. Also my batch_size was 100 as I use a small GPU (only 4 GB of memory).

```

class CoattentionNetSentenceOnly(CoattentionNet):
    def __init__(self, word_inp_size, embedding_size, dropout=0.5):

        super().__init__(word_inp_size, embedding_size, dropout)
    
    def forward(self, image_feat, question_encoding):
        # print("YOU ARE ONLY USING SENTENCES")
        _, _, Qs = self.ques_feat_layer(question_encoding)
    
        image_feat = image_feat.reshape(image_feat.shape[0], 
                                        image_feat.shape[1], -1).permute((0,2,1))
        qhats, vhats = self.word_attention_layer(Qs, image_feat)
        
        hs = self.dropout(self.Ww(qhats + vhats))
        
        p = self.classifier(hs)
        return p
        
```

Reproduce results (Training and plots): 

```python -m student_code.main --log_validation --model coattention --cache_location="./cache" --sentence_only```

Reporduce results (Final Validation accuracy):/

```python -m student_code.main --model coattention --eval_mode --load_checkpoint model_epoch_31000_0.610909090909091_CAsentence --cache_location="./cache" --sentence_only```

**Note:** Make sure checkpoint file is located at ./checkpoints/

![Screenshot%20from%202022-04-22%2021-08-38.png](attachment:Screenshot%20from%202022-04-22%2021-08-38.png)

The green line in the sentence only coattention model, the orange line is the full coattention model from task 3. I have screenshotted it in a way that it is an apples to apples comparison. We can see the *simpler* sentence only model is marginally better through the entire course of training.

I have plotted validation accuracy on 1k examples every 250 iterations as suggested. Best accuracy on 1k subset of 0.6109 (Epoch 12 iteration 1202) was saved during the course of training and is at the google drive link. Best accuracy on full test set was 0.5998 using this best model.

![Screenshot%20from%202022-04-22%2021-09-17.png](attachment:Screenshot%20from%202022-04-22%2021-09-17.png)

**Discussion:** The full coattention model trained for 12 epochs (upto iteration 1192) performed equivalently (final validation accuracy of 0.5987 vs 0.5998) to the sentence only coattention model I tried that trained for 12 epochs (upto iteration 1202).

## Relevant papers

[1] VQA: Visual Question Answering (Agrawal et al, 2016): https://arxiv.org/pdf/1505.00468v6.pdf

[2] Simple Baseline for Visual Question Answering (Zhou et al, 2015): https://arxiv.org/pdf/1512.02167.pdf

[3] Hierarchical Question-Image Co-Attention for Visual Question Answering (Lu et al, 2017):  https://arxiv.org/pdf/1606.00061.pdf

[4] Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering (Goyal, Khot et al, 2017):  https://arxiv.org/pdf/1612.00837.pdf

[5] Stacked Attention Networks for Image Question Answering (Yang et al, 2016): https://arxiv.org/pdf/1511.02274.pdf
